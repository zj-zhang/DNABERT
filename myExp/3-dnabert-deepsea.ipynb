{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d9d197",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08b157dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForSequenceClassification, DNATokenizer\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e069086",
   "metadata": {},
   "source": [
    "# 2. Get DNABERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9e4259",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912a69ef3a5a406b9781c2e943a124d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/8.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FZZ: get model from MODEL_CLASSES\n",
    "config_class, model_class, tokenizer_class = BertConfig, BertForSequenceClassification, DNATokenizer\n",
    "label_list = ['0','1']\n",
    "\n",
    "config = config_class.from_pretrained(\n",
    "    '/mnt/ceph/users/zzhang/DNABERT/myExp/6-new-12w-0',\n",
    "    num_labels=len(label_list),\n",
    "    finetuning_task='dnaprom',\n",
    "    cache_dir=None,\n",
    ")\n",
    "\n",
    "config.hidden_dropout_prob = 0.1\n",
    "config.attention_probs_dropout_prob = 0.1\n",
    "config.split = int(100/512)\n",
    "config.rnn = 'lstm'\n",
    "config.num_rnn_layer = 2\n",
    "config.rnn_dropout = 0\n",
    "config.rnn_hidden = 768\n",
    "config.output_hidden_states = True # add here FZZ\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    'dna6',\n",
    "    do_lower_case=False,\n",
    "    cache_dir=None,\n",
    ")\n",
    "model = model_class.from_pretrained(\n",
    "    '/mnt/ceph/users/zzhang/DNABERT/myExp/6-new-12w-0',\n",
    "    from_tf=False,\n",
    "    config=config,\n",
    "    cache_dir=None,\n",
    ")\n",
    "\n",
    "# _ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed31b31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f2a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "data_dir = \"/mnt/ceph/users/zzhang/workspace_src/AMBER/examples/data/zero_shot_deepsea\"\n",
    "store = h5py.File(os.path.join(data_dir, \"train.h5\"), 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f66d330c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TGCTGCTTTTTCCCCTTAGCCCTGGGCGAGGTCATCATAGAGGGGGAGTGGCAATGGCTCACAAGGTACTAGTGGAACCCCAGTAAGTTATCTCAGAGCCCGCTTAGAACACAAGTGCTACGTCCCCCAAAAGCTTTGCAATGAGTATCTGATGGGAACAAACTCAGTCAAGGACAGGCCCAGGTTGGGGCTTGCAGGCTGCAGATTCAGAATTGTTTATGAGATGGGAGCCATACTTTCTAACAACAAGACCTGAATTTCTCAATTTAATCCAAGTCGTGACTTAAGTTAGCGCCCTTCCGTTCCTCTATTACATTTCTGTTCGGCATGGATCAAATTGCCTACAAGGTGGAACAGATTTCAACTGCAATCTCTGAACCAGAAAATTCACTTATTCTCATGAAAGTTTGTAATCTTTGGAGAGTTGCTTAAACACTTAAAACCATCTTTCCTCTTTCTATACTCCAAACTTACCTGCTGCAATTTCTTGCTAAGAAGCAAAGTGCTATTTGCCTATTCCTATCTCTCTTTACCATCAGACACTCCTTAAGTTAAGAGCTAGATAATTCGCTCAGCCTCAGGCCAGGCCGAGCCTCACTCTAGAAGTCACATTCCTGAGGTGTAGGGGGTCAAAATGCCTCTCATTGTTCAGAAGCAGGTGAGGGGCCAGCCAGGGCACATCCTGCTCTCCAGGCTTGGTTCAGATAACTGTCAGCCCAGTTTTCAAGAGCACACACCAAAAATGCACCAAAGCTTACATCCATACAAACACCCGCACATGGATGTTTATGGAAGCTTATTTGTTTTTATTCATAATCACCCAAACTCAGAATCAACCAAGATGTCCTTCAGTAGATGAATGGATAAACTGTGGTGTGTCCAGGCAGTGGAATATTATTCAACGCAAAAAGAAATGAGCTATCAAGGCATGAAAAAATATGGGGGAACTTTAAATGCATAAATGAGTGAAAGAAGCCAGTCTGAAAAGGCTACACCCCGT'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = store['x'][101]\n",
    "store.close()\n",
    "# double-check DeepSEA letter index; should have a 100% match in hg19\n",
    "''.join(['AGCT'[x] for x in d.argmax(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "648e5455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.processors.utils import DataProcessor, InputExample, InputFeatures\n",
    "\n",
    "\n",
    "class DeepSEA919Processor(DataProcessor):\n",
    "    \"\"\"Processor for the 2015 DeepSEA 919 multi-task data\"\"\"\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"0\", \"1\"] \n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        print(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.h5\")))\n",
    "        return self._create_examples(self._read_h5(os.path.join(data_dir, \"train.h5\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_examples(self._read_h5(os.path.join(data_dir, \"val.h5\")), \"dev\")\n",
    "    \n",
    "    def _read_h5(self, fp):\n",
    "        with h5py.File(fp, 'r') as store:\n",
    "            return zip(*[[self._matrix_to_seq(x, letteridx='AGCT') for x in store['x'][()]], store['y'][()]])\n",
    "    \n",
    "    @staticmethod\n",
    "    def _matrix_to_seq(d, letteridx='ACGT'):\n",
    "        MAX_LEN = 512\n",
    "        s = ''.join([letteridx[x] for x in d.argmax(axis=1)])\n",
    "        slen = len(s)\n",
    "        if slen > MAX_LEN:\n",
    "            s = s[(slen//2-MAX_LEN//2) : (slen//2+MAX_LEN//2)]\n",
    "        return s\n",
    "            \n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        TOKEN_SIZE = 6\n",
    "        for (i, line) in tqdm(enumerate(lines)):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = ' '.join([line[0][i:i+TOKEN_SIZE] for i in range(len(line[0])-TOKEN_SIZE)])\n",
    "            label = str(line[1][0])\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31c13603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOKING AT /mnt/ceph/users/zzhang/workspace_src/AMBER/examples/data/zero_shot_deepsea/train.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4400000it [05:35, 13108.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "processor = DeepSEA919Processor()\n",
    "examples = processor.get_train_examples(data_dir)\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b55c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "pad_on_left = False\n",
    "pad_token = tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]\n",
    "pad_token_segment_id = 0\n",
    "output_mode = 'classification'\n",
    "\n",
    "# from `InputExample` to `InputFeature`, add token_ids and attention masks\n",
    "features = convert_examples_to_features(\n",
    "            examples,\n",
    "            tokenizer,\n",
    "            label_list=label_list,\n",
    "            max_length=max_length,\n",
    "            output_mode=output_mode,\n",
    "            pad_on_left=pad_on_left,  # pad on the left for xlnet\n",
    "            pad_token=pad_token,\n",
    "            pad_token_segment_id=pad_token_segment_id,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ad3985",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76bdc256",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f3dbe",
   "metadata": {},
   "source": [
    "Output = (loss, logits, hidden_states)\n",
    "\n",
    "hidden_states = tuple of 13, each is ordered as below:\n",
    "\n",
    "Annotation:\n",
    "- The layer number (13 layers)\n",
    "- The batch number (1 sentence)\n",
    "- The word / token number (100 tokens in our sentence)\n",
    "- The hidden unit / feature number (768 features)\n",
    "\n",
    "See also: https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b6afc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out(f, data):\n",
    "    for d in data:\n",
    "        f.write(','.join([\"%.5f\"%x for x in d]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32acd701",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275000/275000 [13:09:47<00:00,  5.80it/s]  \n"
     ]
    }
   ],
   "source": [
    "data_iterator = tqdm(dataloader)\n",
    "\n",
    "with open('deepsea_embedding.csv', 'w') as f:\n",
    "    for batch in data_iterator:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        token_emb = torch.stack(\n",
    "            model.bert(input_ids=batch[0], attention_mask=batch[1])[2], \n",
    "            dim=1) # batchsize, 13, 512, 768\n",
    "        token_emb = torch.mean(token_emb[:, -4:, :, :], dim=1) # last 4 embedding layers --> batch, 512, 768\n",
    "        seq_emb = torch.mean(token_emb, dim=1)  # batch, 768\n",
    "        seq_emb = seq_emb.cpu().detach().numpy()\n",
    "        write_out(f, seq_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7245a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = []\n",
    "#for i in tqdm(range(len(features))):\n",
    "#    inputs = features[i]\n",
    "#    outputs.append(model.bert(input_ids=torch.tensor([inputs.input_ids]), attention_mask = torch.tensor([inputs.attention_mask])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c13813",
   "metadata": {},
   "source": [
    "Reload and check if the order still aligns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e089764d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
